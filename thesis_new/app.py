import os
os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"

import streamlit as st
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# --- Constants ---
CHROMA_PATH = "chroma"
PROMPT_TEMPLATE = """
Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ· Î¼ÏŒÎ½Î¿ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎºÎµÎ¯Î¼ÎµÎ½Î¿:

{context}

-------

Î•ÏÏÏ„Î·ÏƒÎ·: {question}
"""

# --- Load Krikri globally ---
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "ilsp/Llama-Krikri-8B-Instruct"

@st.cache_resource
def load_model_and_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )
    model.to(device)
    return tokenizer, model

# --- Vector DB ---
@st.cache_resource
def load_vector_db():
    embedding_function = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
    return Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)


# --- Krikri Generator ---
def generate_with_krikri(tokenizer, model, system_prompt, user_prompt):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

    output = model.generate(
        input_ids,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.9,
        pad_token_id=tokenizer.eos_token_id,
        attention_mask=(input_ids != tokenizer.eos_token_id).long()
    )

    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
    question_position = decoded_output.find(user_prompt.strip())

    if question_position != -1:
        response_text = decoded_output[question_position + len(user_prompt.strip()):].strip()
        return response_text.lstrip("assistant").strip()
    else:
        return decoded_output.strip()


# --- Core Logic ---
def answer_query(query_text, tokenizer, model, db):
    results = db.similarity_search_with_relevance_scores(query_text, k=3)

    if len(results) == 0 or results[0][1] < 0.7:
        return "âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚.", "", []

    context_text = "\n\n---\n\n".join([
        f"URL: {doc.metadata.get('url', 'URL Î»ÎµÎ¯Ï€ÎµÎ¹')}\n"
        f"Î¤Î¯Ï„Î»Î¿Ï‚: {doc.metadata.get('title', 'Î¤Î¯Ï„Î»Î¿Ï‚ Î»ÎµÎ¯Ï€ÎµÎ¹')}\n"
        f"Î™ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´Î±: {doc.metadata.get('website', 'Î™ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´Î± Î»ÎµÎ¯Ï€ÎµÎ¹')}\n"
        f"Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±: {doc.metadata.get('datetime', 'Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î± Î»ÎµÎ¯Ï€ÎµÎ¹')}\n"
        f"Î£Ï„Î®Î»Î·: {doc.metadata.get('section', 'Î£Ï„Î®Î»Î· Î»ÎµÎ¯Ï€ÎµÎ¹')}\n\n"
        f"{doc.page_content}"
        for doc, _ in results
    ])

    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    user_prompt = prompt_template.format(context=context_text, question=query_text)

    system_prompt = (
        "Î•Î¯ÏƒÎ±Î¹ Î­Î½Î± ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ Î±Î½ÎµÏ€Ï„Ï…Î³Î¼Î­Î½Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¤ÎµÏ‡Î½Î·Ï„Î®Ï‚ ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚ Î³Î¹Î± Ï„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ "
        "ÎºÎ±Î¹ Î±Ï€Î±Î½Ï„Î¬Ï‚ ÏƒÎµ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ±Ï†Î¹ÎºÎ¿Ï ÎµÎ½Î´Î¹Î±Ï†Î­ÏÎ¿Î½Ï„Î¿Ï‚ Î¼Îµ Î±Î½Î±Ï†Î¿ÏÎ¬ ÏƒÎµ Ï€Î·Î³Î­Ï‚."
    )

    answer = generate_with_krikri(tokenizer, model, system_prompt, user_prompt)

    # Extract source IDs
    source_ids = [f"{doc.metadata.get('id', 'No ID')}" for doc, _ in results]

    return answer, context_text, source_ids


# --- Main UI ---
def main():
    st.set_page_config(page_title="Krikri Chatbot", page_icon="ğŸ¤–")
    st.title("ğŸ’¬ Krikri Chatbot Î³Î¹Î± Î•Î»Î»Î·Î½Î¹ÎºÎ¬ ÎšÎµÎ¯Î¼ÎµÎ½Î±")

    tokenizer, model = load_model_and_tokenizer()
    db = load_vector_db()

    # Initialize chat history
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # Display previous chat messages
    for role, msg in st.session_state.chat_history:
        with st.chat_message(role):
            st.write(msg)

    # New user input
    user_question = st.chat_input("Î¡ÏÏ„Î± ÎºÎ¬Ï„Î¹ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±...")

    if user_question:
        with st.chat_message("user"):
            st.write(user_question)

        with st.spinner("ğŸ’­ Î£ÎºÎ­Ï†Ï„Î¿Î¼Î±Î¹..."):
            answer, context, source_ids = answer_query(user_question, tokenizer, model, db)

        with st.chat_message("assistant"):
            st.write(answer)

        # Display the source IDs
        if source_ids:
            st.write("ğŸ”— Î Î·Î³Î­Ï‚: ", ", ".join(source_ids))

        # Save to chat history
        st.session_state.chat_history.append(("user", user_question))
        st.session_state.chat_history.append(("assistant", answer))


# --- Entry Point ---
if __name__ == "__main__":
    main()
